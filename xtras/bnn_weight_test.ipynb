{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Class extracted from finnthesizer.py\n",
    "class BNNWeightReader:\n",
    "  def __init__(self, paramFile, interleaveChannels):\n",
    "    self.paramDict = np.load(paramFile)\n",
    "    self.currentParamInd = 0\n",
    "    self.interleaveChannels = interleaveChannels\n",
    "    self.numInterleaveChannels = 0\n",
    "    \n",
    "  def __getCurrent(self):\n",
    "    print(\"arr_{} \".format(self.currentParamInd),end = '')\n",
    "    ret =  self.paramDict[\"arr_\" + str(self.currentParamInd)]\n",
    "    self.currentParamInd += 1\n",
    "    return ret\n",
    "    \n",
    "  def readWeightsRaw(self):\n",
    "    w = self.__getCurrent()\n",
    "    return w\n",
    "    \n",
    "  def readBatchNormLayerRaw(self):\n",
    "    bias = self.__getCurrent()\n",
    "    beta = self.__getCurrent()\n",
    "    gamma = self.__getCurrent()\n",
    "    mean = self.__getCurrent()\n",
    "    invstd = self.__getCurrent()\n",
    "    return (bias, beta, gamma, mean, invstd)\n",
    "    \n",
    "  # read a fully connected layer plus batchnorm, binarize and convert to\n",
    "  # positive threshold form, returning (bin weight matrix, thresholds)\n",
    "  # the returned bin weight matrix has neurons along rows and is suitable\n",
    "  # to be packed into BNN mems using BNNProcElemMem\n",
    "  def readFCBNComplex(self, WPrecisions_fract, APrecisions_fract, IPrecisions_fract, WPrecisions_int, APrecisions_int, IPrecisions_int, numThresBits=16, numThresIntBits=None):\n",
    "    WPrecision = WPrecisions_fract + WPrecisions_int\n",
    "    APrecision = APrecisions_fract + APrecisions_int\n",
    "    IPrecision = IPrecisions_fract + IPrecisions_int\n",
    "    weights = self.readWeightsRaw()\n",
    "    (bias, beta, gamma, mean, invstd) = self.readBatchNormLayerRaw()\n",
    "\n",
    "    if WPrecision==1 and APrecision==1 and IPrecision==1:\n",
    "        (Wb, T) = makeFCBNComplex(weights, bias, beta, gamma, mean, invstd, WPrecisions_int, WPrecisions_fract, use_rowmajor=True)\n",
    "    elif (APrecision==1):\n",
    "        (Wb, T) = makeFCBNComplex(weights, bias, beta, gamma, mean, invstd, WPrecisions_int, WPrecisions_fract, use_rowmajor=True, usePopCount=False)\n",
    "    else:\n",
    "        (Wb, T) = makeFCBNComplex_QNN(weights, bias, beta, gamma, mean, invstd, WPrecisions_fract, APrecisions_fract, WPrecisions_int, APrecisions_int, True, numThresBits, numThresIntBits)\n",
    "    # if the interleave flag is set, permute elements in each row\n",
    "    if self.interleaveChannels and self.numInterleaveChannels != 0:\n",
    "      print(\"Interleaving %d channels in fully connected layer...\" % self.numInterleaveChannels)\n",
    "      pixPerChan = Wb.shape[1] / self.numInterleaveChannels\n",
    "      if (APrecisions_fract == 0):\n",
    "        Wb_perm = np.zeros(Wb.shape, dtype=np.int)\n",
    "      else:\n",
    "        Wb_perm = np.zeros(Wb.shape, dtype=np.float)\n",
    "      for r in range(Wb.shape[0]):\n",
    "        for chan in range(self.numInterleaveChannels):\n",
    "          for cpix in range(pixPerChan):\n",
    "            Wb_perm[r][cpix*self.numInterleaveChannels + chan] = Wb[r][chan*pixPerChan + cpix]\n",
    "      Wb = Wb_perm\n",
    "      # set interleave to zero once we go past this fc layer\n",
    "      self.numInterleaveChannels = 0\n",
    "\n",
    "    return (Wb, T)\n",
    "\n",
    "    # read a fully connected layer without batchnorm and without using thresholds, \n",
    "    # returning bin weight matrix\n",
    "    # the returned bin weight matrix has neurons along rows and is suitable\n",
    "    # to be packed into BNN mems using BNNProcElemMem    \n",
    "  def readFCBNComplex_no_thresholds(self, WPrecisions_fract, APrecisions_fract, IPrecisions_fract, WPrecisions_int, APrecisions_int, IPrecisions_int, numThresBits=16, numThresIntBits=None):\n",
    "    WPrecision = WPrecisions_fract + WPrecisions_int\n",
    "    APrecision = APrecisions_fract + APrecisions_int\n",
    "    IPrecision = IPrecisions_fract + IPrecisions_int\n",
    "    \n",
    "    weights = self.readWeightsRaw()\n",
    "    \n",
    "    #fake the batchnorm params to use same make functions below\n",
    "    bias   = np.zeros(weights.shape[1])    \n",
    "    beta   = np.zeros(weights.shape[1])\n",
    "    gamma  = np.ones(weights.shape[1])\n",
    "    mean   = np.ones(weights.shape[1])\n",
    "    invstd = np.ones(weights.shape[1])\n",
    "\n",
    "    if (WPrecision == 1) and (APrecision == 1) and (IPrecision == 1):\n",
    "        (Wb, T) = makeFCBNComplex(weights, bias, beta, gamma, mean, invstd, WPrecisions_int, WPrecisions_fract, use_rowmajor=True)\n",
    "    elif (APrecision==1):\n",
    "        (Wb, T) = makeFCBNComplex(weights, bias, beta, gamma, mean, invstd, WPrecisions_int, WPrecisions_fract, use_rowmajor=True, usePopCount=False)\n",
    "    else:\n",
    "        (Wb, T) = makeFCBNComplex_QNN(weights, bias, beta, gamma, mean, invstd, WPrecisions_fract, APrecisions_fract, WPrecisions_int, APrecisions_int, True, numThresBits, numThresIntBits)\n",
    "    \n",
    "    # if the interleave flag is set, permute elements in each row\n",
    "    if self.interleaveChannels and self.numInterleaveChannels != 0:\n",
    "        print (\"Interleaving %d channels in fully connected layer...\" % self.numInterleaveChannels)\n",
    "        pixPerChan = Wb.shape[1] / self.numInterleaveChannels\n",
    "        if (APrecisions_fract == 0):\n",
    "            Wb_perm = np.zeros(Wb.shape, dtype=np.int)\n",
    "        else:\n",
    "            Wb_perm = np.zeros(Wb.shape, dtype=np.float)\n",
    "        for r in range(Wb.shape[0]):\n",
    "            for chan in range(self.numInterleaveChannels):\n",
    "                for cpix in range(pixPerChan):\n",
    "                    Wb_perm[r][cpix*self.numInterleaveChannels + chan] = Wb[r][chan*pixPerChan + cpix]\n",
    "        Wb = Wb_perm\n",
    "        # set interleave to zero once we go past this fc layer\n",
    "        self.numInterleaveChannels = 0\n",
    "    \n",
    "    return (Wb, T)\n",
    "    \n",
    "  # read a convolutional layer plus batchnorm, binarize and convert to\n",
    "  # positive threshold form, returning (bin weight matrix, thresholds)\n",
    "  # the returned bin weight matrix  is suitable to be packed into BNN mems \n",
    "  def readConvBNComplex(self, WPrecisions_fract, APrecisions_fract, IPrecisions_fract, WPrecisions_int, APrecisions_int, IPrecisions_int, usePopCount=True,numThresBits=16, numThresIntBits=None):\n",
    "    weights = self.readWeightsRaw()\n",
    "    (bias, beta, gamma, mean, invstd) = self.readBatchNormLayerRaw()\n",
    "    # keep track of output channels for use in FC layer interleave\n",
    "    self.numInterleaveChannels = weights.shape[0]\n",
    "    (Wb, T) = makeConvBNComplex(weights, bias, beta, gamma, mean, invstd, self.interleaveChannels, WPrecisions_fract, APrecisions_fract, IPrecisions_fract, WPrecisions_int, APrecisions_int, IPrecisions_int, usePopCount=usePopCount, numThresBits=numThresBits, numThresIntBits=numThresIntBits)\n",
    "    return (Wb, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_1w1a = BNNWeightReader(\"sfc_mnist-1w-1a.npz\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "arr_0 arr_1 arr_2 arr_3 arr_4 arr_5 \n",
      "weights: (784, 256)\n",
      "bias: (256,)\n",
      "beta: (256,)\n",
      "gamma: (256,)\n",
      "mean: (256,)\n",
      "invstd: (256,)\n",
      "\n",
      "LAYER 1\n",
      "arr_6 arr_7 arr_8 arr_9 arr_10 arr_11 \n",
      "weights: (256, 256)\n",
      "bias: (256,)\n",
      "beta: (256,)\n",
      "gamma: (256,)\n",
      "mean: (256,)\n",
      "invstd: (256,)\n",
      "\n",
      "LAYER 2\n",
      "arr_12 arr_13 arr_14 arr_15 arr_16 arr_17 \n",
      "weights: (256, 256)\n",
      "bias: (256,)\n",
      "beta: (256,)\n",
      "gamma: (256,)\n",
      "mean: (256,)\n",
      "invstd: (256,)\n",
      "\n",
      "LAYER 3\n",
      "arr_18 arr_19 arr_20 arr_21 arr_22 arr_23 \n",
      "weights: (256, 10)\n",
      "bias: (10,)\n",
      "beta: (10,)\n",
      "gamma: (10,)\n",
      "mean: (10,)\n",
      "invstd: (10,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We know beforehand the NN has 4 layers. Read description of 1w1a\n",
    "for i in range(4):\n",
    "    print(\"LAYER {}\".format(i))\n",
    "    weights1 = reader_1w1a.readWeightsRaw()\n",
    "    (bias1, beta1, gamma1, mean1, invstd1) = reader_1w1a.readBatchNormLayerRaw()\n",
    "    print(\"\\nweights: {}\\nbias: {}\\nbeta: {}\\ngamma: {}\\nmean: {}\\ninvstd: {}\\n\".format(weights1.shape,bias1.shape,\n",
    "                                                                         beta1.shape,\n",
    "                                                                         gamma1.shape,\n",
    "                                                                         mean1.shape,\n",
    "                                                                         invstd1.shape))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
